import requests
import re
import concurrent.futures
import datetime
import urllib3

# 1. å±è”½ SSL è­¦å‘Š (é˜²æ­¢æ—¥å¿—åˆ·å±)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- é…ç½®åŒºåŸŸ ---
URLS = [
    # --- å›½å†…ä¼˜è´¨æº (åˆ©ç”¨ä½ çš„ç”µä¿¡IPv6) ---
    "https://raw.githubusercontent.com/dongyubin/IPTV/master/IPTV.m3u",
    "https://raw.githubusercontent.com/fanmingming/live/main/tv/m3u/ipv6.m3u", # èŒƒæ˜Žæ˜Ž IPv6 ç¥žæº
    "https://raw.githubusercontent.com/YueChan/Live/main/IPTV.m3u",
    "https://iptv-org.github.io/iptv/countries/cn.m3u",
    
    # --- å›½é™…ç²¾é€‰æº (åˆ©ç”¨ä½ çš„ Shadowrocket) ---
    "https://iptv-org.github.io/iptv/countries/sg.m3u", # æ–°åŠ å¡ (ä¼˜è´¨)
    "https://iptv-org.github.io/iptv/countries/jp.m3u", # æ—¥æœ¬
    "https://iptv-org.github.io/iptv/countries/gb.m3u", # è‹±å›½
    "https://iptv-org.github.io/iptv/countries/us.m3u"  # ç¾Žå›½
]

OUTPUT_FILE = "playlist.m3u"
MAX_WORKERS = 30 # å¹¶å‘çº¿ç¨‹æ•°
TIMEOUT = 3      # æ£€æµ‹è¶…æ—¶æ—¶é—´(ç§’)

# --- åˆ†ç±»è§„åˆ™ (å…³é”®è¯åŒ¹é…) ---
CATEGORY_RULES = {
    # 1. æœ¬åœ°ç½®é¡¶ (æˆéƒ½ç”µä¿¡ä¸“å±ž)
    "å››å·é¢‘é“": ["å››å·", "æˆéƒ½", "Sichuan", "Chengdu", "åº·å·´", "ç†ŠçŒ«"],
    
    # 2. æ ¸å¿ƒä¸­æ–‡
    "å¤®è§†é¢‘é“": ["CCTV", "å¤®è§†", "CGTN"],
    "å«è§†é¢‘é“": ["å«è§†"],
    "é¦™æ¸¯é¢‘é“": ["ç¿¡ç¿ ", "TVB", "å‡¤å‡°", "HK", "æ˜Žç ", "J2", "Viu"],
    "å°æ¹¾é¢‘é“": ["ä¸­å¤©", "ä¸œæ£®", "æ°‘è§†", "TVBS", "å°è§†", "åŽè§†", "å…¬è§†"],
    
    # 3. çºªå½•ç‰‡ (ä½ çš„æ–°å¢žéœ€æ±‚)
    "çºªå½•ç‰‡": ["çºªå½•", "çºªå®ž", "ç§‘æ•™", "æ¡£æ¡ˆ", "åœ°ç†", "Documentary", "Discovery", "Nat Geo", "History", "Animal", "Planet", "Earth", "Wild"],

    # 4. å›½é™…ç²¾é€‰
    "æ–°åŠ å¡é¢‘é“": ["Singtel", "StarHub", "Mediacorp", "Channel 5", "Channel 8", "CNA"],
    "æ—¥æœ¬é¢‘é“": ["NHK", "Fuji", "TBS", "Asahi", "Nippon", "Tokyo"],
    "å›½é™…æ–°é—»": ["BBC", "CNN", "Fox News", "Sky News", "Al Jazeera", "Bloomberg"],
    "å›½é™…å½±è§†": ["HBO", "Movies", "Cinema", "Film", "Drama", "Warner", "Sony", "AXN"],
    
    # 5. ä½“è‚²ä¸Žæ•°å­—
    "ä½“è‚²é¢‘é“": ["ä½“è‚²", "Sports", "ESPN", "NBA", "Football", "Soccer", "F1"],
    "æ•°å­—é¢‘é“": ["CHC", "å®¶åº­å½±é™¢", "å‰§åœº"]
}

# æœ¬åœ°æºä¿æŠ¤åå• (è·³è¿‡æ£€æµ‹ï¼Œé˜²æ­¢è¯¯åˆ )
KEEP_KEYWORDS = ["å››å·", "æˆéƒ½", "Sichuan", "Chengdu"]

# åžƒåœ¾è¿‡æ»¤é»‘åå•
BLACKLIST = ["è´­ç‰©", "å¤‡ç”¨", "æµ‹è¯•", "Loop", "VOD", "å®£ä¼ ", "å–è¯", "Church", "God", "Religion", "Parliament"]

# --- æ ¸å¿ƒé€»è¾‘ ---

def get_category(name):
    upper_name = name.upper()
    for category, keywords in CATEGORY_RULES.items():
        if not keywords: continue
        for keyword in keywords:
            if keyword.upper() in upper_name:
                return category
    if re.search(r'[\u4e00-\u9fa5]', name): return "å…¶ä»–ä¸­æ–‡"
    return "å›½é™…å…¶ä»–"

def parse_m3u_line(line, current_header):
    try:
        original_name = current_header.split(",")[-1].strip()
        if any(b in original_name for b in BLACKLIST): return None
        new_group = get_category(original_name)
        logo_match = re.search(r'tvg-logo="([^"]+)"', current_header)
        logo_part = f' tvg-logo="{logo_match.group(1)}"' if logo_match else ""
        new_header = f'#EXTINF:-1 group-title="{new_group}" tvg-name="{original_name}"{logo_part},{original_name}'
        return {
            'name': original_name, 'url': line.strip(), 'header': new_header,
            'is_local': any(k in original_name for k in KEEP_KEYWORDS)
        }
    except: return None

def get_channel_items(url):
    channels = []
    try:
        print(f"æ­£åœ¨æŠ“å–: {url}")
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        # verify=False å¿½ç•¥è¯ä¹¦é”™è¯¯
        response = requests.get(url, headers=headers, timeout=15, verify=False)
        response.encoding = 'utf-8'
        if response.status_code == 200:
            lines = response.text.splitlines()
            current_header = ""
            for line in lines:
                line = line.strip()
                if not line: continue
                if line.startswith("#EXTINF"): current_header = line
                elif not line.startswith("#") and current_header:
                    item = parse_m3u_line(line, current_header)
                    if item: channels.append(item)
                    current_header = ""
    except Exception as e: print(f"æŠ“å–å¤±è´¥ {url}: {e}")
    return channels

def check_stream(channel):
    # 1. æœ¬åœ°æºå…æ­» (å››å·/æˆéƒ½)
    if channel['is_local']: return channel
    # 2. å…¶ä»–æºæ£€æµ‹
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        with requests.get(channel['url'], stream=True, headers=headers, timeout=TIMEOUT, verify=False) as response:
            if response.status_code in [200, 302, 405]: return channel
    except: pass
    return None

def main():
    print("ðŸš€ ä»»åŠ¡å¼€å§‹...")
    all_channels = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(get_channel_items, url) for url in URLS]
        for future in concurrent.futures.as_completed(futures): all_channels.extend(future.result())

    if not all_channels: return

    # åŽ»é‡
    unique_channels = {}
    for ch in all_channels:
        url = ch['url']
        if url not in unique_channels: unique_channels[url] = ch
        else:
            if ("HD" in ch['name']) and ("HD" not in unique_channels[url]['name']): unique_channels[url] = ch
    
    work_list = list(unique_channels.values())
    print(f"âœ… åŽ»é‡å®Œæˆï¼Œå…± {len(work_list)} ä¸ªé¢‘é“ã€‚å¼€å§‹æ£€æµ‹...")

    valid_channels = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_channel = {executor.submit(check_stream, ch): ch for ch in work_list}
        for future in concurrent.futures.as_completed(future_to_channel):
            res = future.result()
            if res: valid_channels.append(res)

    # æŽ’åºä¼˜å…ˆçº§
    group_priority = ["å››å·é¢‘é“", "å¤®è§†é¢‘é“", "å«è§†é¢‘é“", "çºªå½•ç‰‡", "é¦™æ¸¯é¢‘é“", "æ–°åŠ å¡é¢‘é“", "å°æ¹¾é¢‘é“", "ä½“è‚²é¢‘é“", "æ—¥æœ¬é¢‘é“", "å›½é™…æ–°é—»", "å›½é™…å½±è§†"]
    
    def sort_key(ch):
        g_match = re.search(r'group-title="([^"]+)"', ch['header'])
        group = g_match.group(1) if g_match else "å…¶ä»–é¢‘é“"
        try: g_score = group_priority.index(group)
        except: g_score = 99
        is_ipv6 = 0 if ('[' in ch['url'] or ch['url'].count(':') > 2) else 1
        return (g_score, is_ipv6, len(ch['name']))

    valid_channels.sort(key=sort_key)

    if len(valid_channels) < 10: return

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        # å†™å…¥ EPG åœ°å€
        f.write('#EXTM3U x-tvg-url="http://epg.51zmt.top:8000/e.xml"\n')
        f.write(f"# Updated at {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        for channel in valid_channels:
            f.write(f"{channel['header']}\n")
            f.write(f"{channel['url']}\n")
    
    print(f"ðŸŽ‰ æˆåŠŸï¼ç”Ÿæˆ {len(valid_channels)} ä¸ªé¢‘é“ã€‚")

if __name__ == "__main__":
    main()
